# Hongong Machin Learning and Deep Learning chapter 6
## 6-2. K-평균  
#### * 책 페이지: "혼자공부하는 머신러닝 + 딥러닝" P303 ~ P317
#### * 참고 코드: "6_2_K_Means.ipynb"
### 과정 
1. K-평균 알고리즘 소개 
2. KMeans 클래스 
3. 클러스터 중심
4. 최적의 K 찾기 
### 정리 및 결론
- 과정 정리 
  - 6-1에서는 과일을 종류별로 픽셀 평균값을 계산했다. 하지만 실전에서는 어떤 과일 사진이 들어올지 모른다. 따라서 타깃값을 모르는 척 하고 자동으로 사진을 클러스터로 모을 수 있는 군집 알고리즘이 필요하다.
  - 여기에서는 대표적인 군집 알고리즘인 K-평균 알고리즘을 사용했다. K-평균은 비교적 간단하고 속도가 빠르며 이해하기도 쉽다. K-평균 알고리즘을 구현한 사이킷런의 KMeans 클래스는 각 샘플이 어떤 클러스터에 소속되어 있는지 labels_ 속성에 저장한다.
  - 각 샘플에서 각 클러스터까지의 거리를 하나의 특성으로 활용할 수도 있다. 이를 위해 KMeans 클래스는 transform() 메서드를 제공한다. 또한 predict() 메서드에서 새로운 샘플에 대해 가장 가까운 클러스터를 예측값으로 출력한다.
  - K-평균 알고리즘은 사전에 클러스터 개수를 미리 지정해야 한다. 사실 데이터를 직접 확인하지 않고서는 몇 개의 클러스터가 만들어질지 알기 어렵다. 최적의 클러스터 개수 k를 알아내는 한 가지 방법은 클러스터가 얼마나 밀접되어 있는지 나타내는 이너셔를 사용하는 것이다. 이너셔가 더 이상 크게 줄어들지 않는다면 클러스터 개수를 더 늘리는 것은 효과가 없다.이를 엘보우 방법이라고 부른다.
  - 사이킷런의 KMeans 클래스는 자동으로 이너셔를 계산하여 inertia_ 속성으로 제공한다. 클러스터 개수를 늘리면서 반복하여 KMeans 알고리즘을 훈련하고 이너셔가 줄어드는 속도가 꺾이는 지점을 최적의 클러스터 개수로 결정한다.
  - 여기서 K-평균 알고리짐의 클러서트 중심까지 거리를 특성으로 사용할 수도 있다는 점을 보았다. 이렇게 하면 훈련 데이터의 차원을 크게 줄일 수 있다. 데이터셋의 차원을 줄이면 지도 학습 알고리즘의 속도를 크게 높일 수 있다. 다음에는 비지도 학습의 또다른 종류인 차원 축소에 대해 본격적으로 알아본다.
