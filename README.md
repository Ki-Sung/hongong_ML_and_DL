# Hongong Machin Learning and Deep Learning chapter 6
## 6-1. 군집 알고리즘  
#### * 책 페이지: "혼자공부하는 머신러닝 + 딥러닝" P286 ~ P302
#### * 참고 코드: "6_1_Clustering_Algorithm.ipynb"
### 과정 
1. 타깃을 모르는 비지도 학습 
2. 과일 사진 데이터 준비하기 
3. 픽셀값 분석하기
4. 평군값과 가까운 사진 고르기
### 정리 및 결론
- 과정 정리 
  - 타깃값이 없을 때 데이터에 있는 패턴을 찾거나 데이터 구조를 파악하는 머신러닝 방식을 비지도 학습이라고 한다.
  - 타깃이 없기 때문에 알고리즘을 직접적으로 가르칠 수가 없다. 대신 알고리즘은 스스로 데이터가 어떻게 구성되어 있는지 분석한다.
  - 대표적인 비지도 학습 문제는 '군집'이다. 군집은 비슷한 샘플끼리 그룹으로 모으는 작업을 말한다. 이 절에서는 사진의 픽섹을 사용해 군집과 비슷한 작업을 수행해 보았다.
  - 하지만 샘플이 어떤 과일인지 미리 알고 있었기 때문에 사과 사진의 평균값을 알 수 있었다. 실제 비지도 학습에서는 타깃이 없는 사진을 사용해야 한다. (다음 절에서 이런 경우 어떻게 샘플 그룹의 평균값을 찾는지 알아본다.)
## 6-2. K-평균  
#### * 책 페이지: "혼자공부하는 머신러닝 + 딥러닝" P303 ~ P317
#### * 참고 코드: "6_2_K_Means.ipynb"
### 과정 
1. K-평균 알고리즘 소개 
2. KMeans 클래스 
3. 클러스터 중심
4. 최적의 K 찾기 
### 정리 및 결론
- 과정 정리 
  - 6-1에서는 과일을 종류별로 픽셀 평균값을 계산했다. 하지만 실전에서는 어떤 과일 사진이 들어올지 모른다. 따라서 타깃값을 모르는 척 하고 자동으로 사진을 클러스터로 모을 수 있는 군집 알고리즘이 필요하다.
  - 여기에서는 대표적인 군집 알고리즘인 K-평균 알고리즘을 사용했다. K-평균은 비교적 간단하고 속도가 빠르며 이해하기도 쉽다. K-평균 알고리즘을 구현한 사이킷런의 KMeans 클래스는 각 샘플이 어떤 클러스터에 소속되어 있는지 labels_ 속성에 저장한다.
  - 각 샘플에서 각 클러스터까지의 거리를 하나의 특성으로 활용할 수도 있다. 이를 위해 KMeans 클래스는 transform() 메서드를 제공한다. 또한 predict() 메서드에서 새로운 샘플에 대해 가장 가까운 클러스터를 예측값으로 출력한다.
  - K-평균 알고리즘은 사전에 클러스터 개수를 미리 지정해야 한다. 사실 데이터를 직접 확인하지 않고서는 몇 개의 클러스터가 만들어질지 알기 어렵다. 최적의 클러스터 개수 k를 알아내는 한 가지 방법은 클러스터가 얼마나 밀접되어 있는지 나타내는 이너셔를 사용하는 것이다. 이너셔가 더 이상 크게 줄어들지 않는다면 클러스터 개수를 더 늘리는 것은 효과가 없다.이를 엘보우 방법이라고 부른다.
  - 사이킷런의 KMeans 클래스는 자동으로 이너셔를 계산하여 inertia_ 속성으로 제공한다. 클러스터 개수를 늘리면서 반복하여 KMeans 알고리즘을 훈련하고 이너셔가 줄어드는 속도가 꺾이는 지점을 최적의 클러스터 개수로 결정한다.
  - 여기서 K-평균 알고리짐의 클러서트 중심까지 거리를 특성으로 사용할 수도 있다는 점을 보았다. 이렇게 하면 훈련 데이터의 차원을 크게 줄일 수 있다. 데이터셋의 차원을 줄이면 지도 학습 알고리즘의 속도를 크게 높일 수 있다. 다음에는 비지도 학습의 또다른 종류인 차원 축소에 대해 본격적으로 알아본다.
## 6-3. 주성분 분석  
#### * 책 페이지: "혼자공부하는 머신러닝 + 딥러닝" P318 ~ P337
#### * 참고 코드: "6_3_PCA(Principal_Component_Analysis).ipynb"
### 과정 
1. 차원과 차원 축소
2. 주성분 분석 소개 
3. PCA 클래스 
4. 원본 데이터 재구성
5. 설명된 분산
6. 다른 알고리즘과 함께 사용하기
### 정리 및 결론
- 과정 정리 
  - 6-3에서는 대표적인 비지도 학습 문제 중 하나인 차원 축소에 대해 알아보았다. 차원 축소를 사용하면 데이터셋의 크기를 줄일 수 있고 비교적 시각화하기 쉽다. 또 차원 축소된 데이터를 지도 학습 알고리즘이나 다른 비지도 학습 알고리즘에 재사용하여 성능을 높이거나 훈련 속도를 빠르게 만들 수 있다.
  - 사이킷런의 PCA 클래스를 사용해 과일 사진 데이터의 특성(차원)을 50개로 크게 줄였다. 특성(차원) 개수는 작지만 변환된 데이터는 원본 데이터에 있는 분산의 90% 이상을 표현한다. 이를 설명된 분산 이라고 부른다.
  - PCA 클래스는 자동으로 설명된 분산을 계산하여 제공해 준다. 또한 주성분의 개수를 명시적으로 지정하는 대신 설명된 분산의 비율을 설정하여 원하는 비율만큼 주성분을 찾을 수 있다.
  - PCA 클래스는 변환된 데이터에서 원본 데이터를 복원하는 메서드도 제공한다. 변환된 데이터가 원본 데이터의 분산을 모두 유지하고 있지 않다면 완벽하게 복원되지 않는다. 하지만 적은 특성(차원)으로도 상당 부분의 디테일을 복원할 수 있다.
