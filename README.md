# Hongong Machin Learning and Deep Learning chapter 5
## 5-1. 결정 트리 
#### * 책 페이지: "혼자공부하는 머신러닝 + 딥러닝" P220 ~ P241
#### * 참고 코드: "5-1_decision_tree.ipynb"
### 과정 
- 로지스틱 회귀로 와인 분류하기
  - 설명하기 쉬운 모델과 어려운 모델 
- 결정트리 
  - 불순도
  - 가지치기 
### 정리 및 결론
- 과정 정리 
  - 가을 신상품으로 준비한 캔 와인은 실수로 와인의 종류(레드 와인 / 화이트 와인)가 캔에 인쇄되지 않았다.
  - 알코올 도수, 당도, pH 데이터를 기준으로 화이트 와인을 골라내는 이진 분류 로지스틱 회귀 모델을 훈련 했지만, 다른 사람이 보고서를 이해할 수 없었다.
  - 그래서 결정 트리를 사용해 레드 와인과 화이트 와인을 분류하는 문제를 풀었다.
  - 특성을 더 추가하지 않고도 결정 트리의 성능이 로지스틱 회귀 모델보다 더 좋음을 알 수 있었다. 게다가 결정 트리는 깊이가 너무 깊지 않다면 비교적 설명하기 쉬웠다.
  - 그리고 결정 트리가 어떻게 데이터를 분할하는지 이해하기 위해 불순도 개념과 정보 이득에 대해 알아보았다.
- 결과
  - 머신러닝 모델을 종종 블랙박스와 같다고도 말한다. 실제로 모델의 계수나 절편이 왜 그렇게 학습되었는지 설명하기가 어렵다.
  - 하지만 결정 트리는 비교적 비전문가에게도 설명하기 쉬운 모델을 만들어 낸다.
  - 결정 트리는 여기서 끝이 아니다. 결정 트리는 많은 앙상블 학습 알고리즘의 기반이 된다. (앙상블 학습은 신경망과 함께 가장 높은 성능을 내기 때문에 인기가 높은 알고리즘이다.)  
## 5-2. 교차 검증과 그리드 서치
#### * 책 페이지: "혼자공부하는 머신러닝 + 딥러닝" P242 ~ P262
#### * 참고 코드: "5-2_Cross_Validation_and_Grid_search.ipynb"
### 과정 
- 검증 세트 
- 교차 검증 
  - 분할기를 사용한 교차 검증
- 하이퍼파라미터 튜닝
  - 그리드 서치 
  - 랜덤 서치 
### 정리 및 결론
- 과정 정리 
  - 레드 와인과 화이트 와인을 선별하는 작업의 성능을 끌어올리기 위해 결정 트리의 다양한 하이퍼파라미터를 시도해 봐야 한다. 이런 과정에서 테스트 세트를 사용하면 결국 테스트 세트에 맞춰 모델을 훈련하는 효과를 만들어 낸다.
  - 테스트 세트는 최종 모델을 선택할 때까지 사용하지 않아야 한다. 테스트 세트를 사용하지 않고 모델을 평가하려면 또 다른 세트가 필요하다. 이를 검증 세트라고 부른다.
  - 검증 세트는 훈련 세트 중 일부를 다시 덜어 내어 만든다.
  - 검증 세트가 크지 않다면 어떻게 데이터를 나누었는지에 따라 검증 점수가 들쭉날쭉할 것이다. 훈련한 모델의 선응을 안정적으로 평가하기 위해 검증 세트를 한 번 나누어 모델을 평가하는 것에 그치지 않고, 여러 번 반복할 수 있는데, 이를 교차 검증이라고 한다.
  - 보통 훈련 세트를 5등분 혹은 10등분 한다. 나누어진 한 덩어리를 폴드라고 부르며 한 폴드씩 돌아가면서 검증 세트의 역할을 한다. 따라서 전체적으로 5개 혹은 10개의 모델을 만든다.
  - 교차 검증을 사용한 다양한 하이퍼파라미터를 탐색한다. 머신러닝 라이브러리에서는 클래스와 메서드의 매개변수를 바꾸어 모델을 훈련하고 평가해 보는 작업이다.
  - 이런 과정은 때론 지루하고 반복적이다. 테스트하고 싶은 매개변수 리스트를 만들어 이 과정을 자동화하는 그리드 서치를 사용하면 편리하다.
  - 매개변수 값이 수치형이고 특히 연속적인 실수값이라면 싸이파이의 확률 분포 객체를 전달하여 특성 범위 내에서 지정된 횟수만큼 매개변수 후보 값을 샘플링하여 교차 검증을 시도할 수 있다.
  - 이는 한정된 자원을 최대한 활용하여 효율적으로 하이퍼파라미터 공간을 탐샐할 수 있는 아주 좋은 도구이다.
