# Hongong Machin Learning and Deep Learning chapter 4
## 4-1. 로지스틱 회귀 
#### * 책 페이지: "혼자공부하는 머신러닝 + 딥러닝" P176 ~ P198
#### * 참고 코드: "4-1_Logistic_Regression.ipynb"
### 과정 
- 럭키백 확률
  - 데이터 준비하기
  - K-최근접 이웃 분류기의 확률 예측
- 로지스틱 회귀 
  - 로지스틱 회귀로 이진 분류 수행하기
  - 로지스틱 회귀로 다중 분류 수행하기
### 정리 및 결론
- 과정 정리 
  - 이번 공부는 럭키백에 담긴 생선이 어떤 생선인지 확률을 예측하는 것이었다.
  - K-최근접 이웃 모델의 확률을 출력할 수 있지만 이웃한 샘플의 클래스 비율이므로 항상 정해진 확률만 출력한다.
  - 이를 위해 가장 대표적인 분류 알고리즘 중 하나인 로지스틱 회귀를 사용하였다. 로지스틱 회귀는 회귀 모델이 아닌 분류 모델이다.
  - 로지스틱 회귀는 선형 회귀 처럼 선형 방정식을 사용하지만 선형 회귀처럼 계산한 값을 그대로 출력하는 것이 아니라, 0 - 1 사이로 압축을 한다.
  - 로지스틱 회귀 이진 분류에서는 하나의 선형 방정식을 훈련한다. 이 방정식의 출력값을 시그모이드 함수에 통과시켜 0 - 1 사이의 값을 만든다. 이 값이 양성 클래스에 대한 확률이다. 음성 클래스의 확률은 1에서 양성 클래스의 확률을 빼면 된다.
  - 다중 분류일 경우 클래스 개수만큼 방정식을 훈련한다. 그다음 각 방정식의 출력밧을 소프트맥스 함수를 통과시켜 전체 클래스에 대한 합이 항상 1이 되도록 만든다. 이 값을 각 클래스에 대한 확률로 이해할 수 있다.
## 4-2. 확률적 경사 하강법
#### * 책 페이지: "혼자공부하는 머신러닝 + 딥러닝" P199 ~ P217
#### * 참고 코드: "4-2_Stochastic Gradient Descent.ipynb"
### 과정 
- 점진적 학습 
  - 확률적 경사 하강법
  - 손실 함수
  - 로지스틱 손실 함수 
- SGD Classifier
- 에포크와 과소/과대적합
### 정리 및 결론 
- 과정 정리 
  - 이번 공부는 럭키백의 폭박적인 인기에 힘입어 생선을 실시간으로 학습하기 위한 새로운 머신러닝 모델이 필요하여, **확률적 경사 하강법**을 사용해 점진적으로 학습하는 **로지스틱 회귀 모델**을 훈련하였다.
  - **확률적 경사 하강법**은 손실 함수라는 산을 정의하고 가장 가파른 경사를 따라 조금씩 내려오는 알고리즘이다.
  - 충분히 반복하여 훈련하면 훈련 세트에서 높은 점수를 얻는 모델을 만들 수 있다. 하지만 훈련을 반복할수록 모델이 훈련 세트에 점점 더 잘 맞게 되어 어느 순간 과대적합되고 테스트 세트의 정확도가 줄어든다.
  - 요즘에는 대량의 데이터를 이용하여 문제를 해결해야 하는 일이 흔한데, 이런 넘쳐나는 데이터가 머신러닝와 인공지능의 발전에 크게 기여했다. 데이터가 매우 크기 때문에 전통적인 머신러닝 방식으로 모델을 만들기가 어려워졌다. 데이터를 한 번에 모두 컴퓨터 메모리에 읽을 수 없기 때문이다.
  - 따라서 데이터를 조금씩 사용해 점진적으로 학습하는 방법이 필요해졌다. **확률적 경사 하강법**이 바로 이 문제를 해결하는 핵심 열쇠이다. (뒤에 신경망 알고리즘에서 더 자세 다룰 예정)
  - 그리고 **확률적 경사 하강법**은 머신러닝, 딥러닝 알고리즘이 아니라, 머신러닝과 딥러닝 알고리즘을 훈련, 즉 최적화하기 위한 방법이라는 것을 명심하자.
